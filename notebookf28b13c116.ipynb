{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10826240,"sourceType":"datasetVersion","datasetId":6722586}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n# Load the pre-trained model (MobileNetV2) without the top layer\nbase_model = keras.applications.MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\n# Freeze the base model\nbase_model.trainable = False\n\n# Create a new top model\ninputs = keras.Input(shape=(224, 224, 3))\nx = base_model(inputs, training=False)\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512, activation='relu')(x)\nx = layers.BatchNormalization()(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(10, activation='softmax')(x)  # Adjust the number of classes\n\n# Create the final model\nmodel = keras.Model(inputs, outputs)\n\n# Compile the model\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Load your dataset (replace with your dataset)\ndatagen = keras.preprocessing.image.ImageDataGenerator(validation_split=0.2, rescale=1./255)\ntrain_data = datagen.flow_from_directory('/kaggle/input/notebook50', target_size=(224, 224), batch_size=32, subset='training')\nval_data = datagen.flow_from_directory('/kaggle/input/notebook50', target_size=(224, 224), batch_size=32, subset='validation')\n\n# Train the model\nmodel.fit(train_data, validation_data=val_data, epochs=20)\n\n# Unfreeze some layers and fine-tune\nbase_model.trainable = True\nfor layer in base_model.layers[:-30]:  # Unfreezing last 30 layers\n    layer.trainable = False\n\n# Compile again with a lower learning rate\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Continue training\nmodel.fit(train_data, validation_data=val_data, epochs=15)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-22T22:30:49.412746Z","iopub.execute_input":"2025-02-22T22:30:49.413128Z","iopub.status.idle":"2025-02-22T22:44:17.289508Z","shell.execute_reply.started":"2025-02-22T22:30:49.413089Z","shell.execute_reply":"2025-02-22T22:44:17.288241Z"}},"outputs":[{"name":"stdout","text":"Found 589 images belonging to 1 classes.\nFound 147 images belonging to 1 classes.\nEpoch 1/20\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1s/step - accuracy: 0.1295 - loss: 3.1976 - val_accuracy: 0.0000e+00 - val_loss: 3.4195\nEpoch 2/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 969ms/step - accuracy: 0.2927 - loss: 2.2528 - val_accuracy: 0.0340 - val_loss: 2.3718\nEpoch 3/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.4676 - loss: 1.6431 - val_accuracy: 0.9932 - val_loss: 0.9596\nEpoch 4/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 976ms/step - accuracy: 0.7805 - loss: 0.9077 - val_accuracy: 1.0000 - val_loss: 0.7345\nEpoch 5/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 960ms/step - accuracy: 0.9435 - loss: 0.4824 - val_accuracy: 1.0000 - val_loss: 0.3039\nEpoch 6/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 987ms/step - accuracy: 0.9841 - loss: 0.2230 - val_accuracy: 1.0000 - val_loss: 0.0791\nEpoch 7/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9956 - loss: 0.1253 - val_accuracy: 1.0000 - val_loss: 0.0238\nEpoch 8/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0677 - val_accuracy: 1.0000 - val_loss: 0.0166\nEpoch 9/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 968ms/step - accuracy: 0.9989 - loss: 0.0495 - val_accuracy: 1.0000 - val_loss: 0.0159\nEpoch 10/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 956ms/step - accuracy: 0.9943 - loss: 0.0438 - val_accuracy: 1.0000 - val_loss: 0.0087\nEpoch 11/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9993 - loss: 0.0222 - val_accuracy: 1.0000 - val_loss: 0.0068\nEpoch 12/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 983ms/step - accuracy: 1.0000 - loss: 0.0201 - val_accuracy: 1.0000 - val_loss: 0.0045\nEpoch 13/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 951ms/step - accuracy: 0.9987 - loss: 0.0161 - val_accuracy: 1.0000 - val_loss: 0.0026\nEpoch 14/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0135 - val_accuracy: 1.0000 - val_loss: 0.0024\nEpoch 15/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 962ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0031\nEpoch 16/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0111 - val_accuracy: 1.0000 - val_loss: 0.0028\nEpoch 17/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 984ms/step - accuracy: 1.0000 - loss: 0.0092 - val_accuracy: 1.0000 - val_loss: 0.0018\nEpoch 18/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 947ms/step - accuracy: 1.0000 - loss: 0.0064 - val_accuracy: 1.0000 - val_loss: 0.0016\nEpoch 19/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0070 - val_accuracy: 1.0000 - val_loss: 0.0017\nEpoch 20/20\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0063 - val_accuracy: 1.0000 - val_loss: 0.0017\nEpoch 1/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 1s/step - accuracy: 0.9930 - loss: 0.0543 - val_accuracy: 1.0000 - val_loss: 0.0236\nEpoch 2/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9959 - loss: 0.0372 - val_accuracy: 1.0000 - val_loss: 0.0281\nEpoch 3/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9954 - loss: 0.0284 - val_accuracy: 1.0000 - val_loss: 0.0129\nEpoch 4/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9923 - loss: 0.0365 - val_accuracy: 1.0000 - val_loss: 0.0278\nEpoch 5/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9963 - loss: 0.0209 - val_accuracy: 1.0000 - val_loss: 0.0600\nEpoch 6/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0156 - val_accuracy: 1.0000 - val_loss: 0.0395\nEpoch 7/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.9997 - loss: 0.0098 - val_accuracy: 1.0000 - val_loss: 0.0217\nEpoch 8/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0175 - val_accuracy: 1.0000 - val_loss: 0.0189\nEpoch 9/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9974 - loss: 0.0112 - val_accuracy: 1.0000 - val_loss: 0.0094\nEpoch 10/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9966 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0174\nEpoch 11/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0041 - val_accuracy: 1.0000 - val_loss: 0.0122\nEpoch 12/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0108 - val_accuracy: 1.0000 - val_loss: 0.0082\nEpoch 13/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0073 - val_accuracy: 1.0000 - val_loss: 0.0090\nEpoch 14/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0111\nEpoch 15/15\n\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 0.0052 - val_accuracy: 1.0000 - val_loss: 0.0154\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x79b850437520>"},"metadata":{}}],"execution_count":9}]}